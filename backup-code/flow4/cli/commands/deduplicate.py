"""
CLI command for dataset deduplication.

This command provides comprehensive deduplication capabilities for Flow4 datasets,
including RAG datasets, instruction datasets, and synthetic datasets.
"""

import argparse
import os
from pathlib import Path
from typing import List, Dict, Any

from ...utils.logging import get_logger
from ...utils.deduplication import deduplicate_instruction_dataset, deduplicate_rag_datasets

logger = get_logger(__name__)


class DeduplicateCommand:
    """CLI command for dataset deduplication."""
    
    def add_parser(self, subparsers, name: str) -> argparse.ArgumentParser:
        """Add the deduplicate subcommand parser."""
        parser = subparsers.add_parser(
            name,
            help='Deduplicate Flow4 datasets to remove duplicate entries',
            description="""
Comprehensive dataset deduplication for Flow4 outputs.

This command can deduplicate various types of datasets generated by Flow4:
- RAG datasets (chunks, Q&A pairs, embeddings)
- Instruction datasets for fine-tuning
- Synthetic datasets from LLM generation

Deduplication strategies:
- exact: Remove exact string matches
- normalized: Remove matches after text normalization
- hash: Remove content-based duplicates
- semantic: Remove semantically similar items (requires sentence-transformers)
- progressive: Apply multiple strategies sequentially

Examples:
  # Deduplicate a single dataset file
  flow4 deduplicate --input output/instruction_dataset.json --output output/deduplicated_instruction_dataset.json

  # Deduplicate all datasets in a directory
  flow4 deduplicate --input output/rag/ --output output/deduplicated_rag/ --directory

  # Use semantic deduplication with custom threshold
  flow4 deduplicate --input dataset.json --strategy semantic --similarity-threshold 0.85

  # Batch deduplicate with detailed reporting
  flow4 deduplicate --input output/ --output output_clean/ --directory --strategy progressive --verbose
            """,
            formatter_class=argparse.RawDescriptionHelpFormatter
        )
        
        # Input/Output options
        io_group = parser.add_argument_group('Input/Output')
        io_group.add_argument(
            '--input', '-i',
            required=True,
            help='Input dataset file or directory'
        )
        io_group.add_argument(
            '--output', '-o',
            help='Output file or directory (default: add _deduplicated suffix)'
        )
        io_group.add_argument(
            '--directory', '-d',
            action='store_true',
            help='Process all dataset files in directory'
        )
        io_group.add_argument(
            '--pattern',
            default='*.json,*.jsonl',
            help='File patterns to process in directory mode (default: *.json,*.jsonl)'
        )
        
        # Deduplication options
        dedup_group = parser.add_argument_group('Deduplication Settings')
        dedup_group.add_argument(
            '--strategy', '-s',
            choices=['exact', 'normalized', 'hash', 'semantic', 'progressive'],
            default='progressive',
            help='Deduplication strategy (default: progressive)'
        )
        dedup_group.add_argument(
            '--similarity-threshold',
            type=float,
            default=0.95,
            help='Similarity threshold for semantic deduplication (0.0-1.0, default: 0.95)'
        )
        dedup_group.add_argument(
            '--keys',
            help='Comma-separated list of keys to check for duplicates (auto-detected if not specified)'
        )
        
        # Output options
        output_group = parser.add_argument_group('Output Options')
        output_group.add_argument(
            '--in-place',
            action='store_true',
            help='Modify files in place (overwrite original files)'
        )
        output_group.add_argument(
            '--dry-run',
            action='store_true',
            help='Show what would be deduplicated without making changes'
        )
        output_group.add_argument(
            '--report-only',
            action='store_true',
            help='Generate deduplication reports without saving deduplicated datasets'
        )
        
        # Advanced options
        advanced_group = parser.add_argument_group('Advanced')
        advanced_group.add_argument(
            '--backup',
            action='store_true',
            help='Create backup of original files before deduplication'
        )
        advanced_group.add_argument(
            '--force',
            action='store_true',
            help='Overwrite existing output files without confirmation'
        )
        
        return parser
    
    def run(self, args: argparse.Namespace) -> int:
        """Execute the deduplicate command."""
        try:
            logger.info("üîç Starting dataset deduplication...")
            logger.info(f"   Input: {args.input}")
            logger.info(f"   Strategy: {args.strategy}")
            
            input_path = Path(args.input)
            
            # Validate input
            if not input_path.exists():
                logger.error(f"‚ùå Input path does not exist: {input_path}")
                return 1
            
            # Determine output path
            if args.output:
                output_path = Path(args.output)
            elif args.in_place:
                output_path = input_path
            else:
                if input_path.is_file():
                    output_path = input_path.parent / f"{input_path.stem}_deduplicated{input_path.suffix}"
                else:
                    output_path = input_path.parent / f"{input_path.name}_deduplicated"
            
            # Parse keys if provided
            check_keys = None
            if args.keys:
                check_keys = [k.strip() for k in args.keys.split(',')]
            
            # Process based on input type
            if args.directory or input_path.is_dir():
                return self._deduplicate_directory(args, input_path, output_path, check_keys)
            else:
                return self._deduplicate_file(args, input_path, output_path, check_keys)
        
        except Exception as e:
            logger.error(f"‚ùå Deduplication failed: {e}")
            if args.debug:
                import traceback
                logger.error(traceback.format_exc())
            return 1
    
    def _deduplicate_file(
        self, 
        args: argparse.Namespace, 
        input_path: Path, 
        output_path: Path,
        check_keys: List[str] = None
    ) -> int:
        """Deduplicate a single file."""
        logger.info(f"üìÑ Processing file: {input_path.name}")
        
        # Create backup if requested
        if args.backup and not args.dry_run and not args.report_only:
            backup_path = input_path.parent / f"{input_path.stem}_backup{input_path.suffix}"
            import shutil
            shutil.copy2(input_path, backup_path)
            logger.info(f"üíæ Backup created: {backup_path}")
        
        # Check if output exists
        if output_path.exists() and not args.force and not args.in_place:
            logger.error(f"‚ùå Output file exists: {output_path}")
            logger.error("Use --force to overwrite or --in-place to modify original")
            return 1
        
        try:
            if args.dry_run:
                logger.info("üîç Dry run - analyzing duplicates...")
                # TODO: Implement dry run analysis
                logger.info("‚úÖ Dry run completed")
                return 0
            
            if args.report_only:
                output_file = None
            else:
                output_file = str(output_path)
            
            # Deduplicate the file
            report = deduplicate_instruction_dataset(
                str(input_path),
                output_file,
                args.strategy
            )
            
            # Display results
            self._display_report(report, input_path.name)
            
            if not args.report_only:
                logger.info(f"‚úÖ Deduplicated dataset saved to: {output_path}")
            
            return 0
            
        except Exception as e:
            logger.error(f"‚ùå Failed to deduplicate {input_path}: {e}")
            return 1
    
    def _deduplicate_directory(
        self, 
        args: argparse.Namespace, 
        input_path: Path, 
        output_path: Path,
        check_keys: List[str] = None
    ) -> int:
        """Deduplicate all files in a directory."""
        logger.info(f"üìÅ Processing directory: {input_path}")
        
        # Parse file patterns
        patterns = [p.strip() for p in args.pattern.split(',')]
        
        # Find files to process
        files_to_process = []
        for pattern in patterns:
            files_to_process.extend(input_path.glob(pattern))
        
        if not files_to_process:
            logger.warning(f"‚ö†Ô∏è No files found matching patterns: {patterns}")
            return 0
        
        logger.info(f"üìã Found {len(files_to_process)} files to process")
        
        # Create output directory if needed
        if not args.in_place and not args.dry_run and not args.report_only:
            output_path.mkdir(parents=True, exist_ok=True)
        
        # Process each file
        success_count = 0
        total_reports = {}
        
        for file_path in files_to_process:
            try:
                if args.in_place:
                    file_output_path = file_path
                else:
                    file_output_path = output_path / file_path.name
                
                logger.info(f"üîÑ Processing {file_path.name}...")
                
                if self._deduplicate_file(args, file_path, file_output_path, check_keys) == 0:
                    success_count += 1
                
            except Exception as e:
                logger.error(f"‚ùå Failed to process {file_path}: {e}")
        
        # Summary
        logger.info(f"üìä Directory processing complete:")
        logger.info(f"   ‚úÖ Successful: {success_count}/{len(files_to_process)}")
        
        if success_count == len(files_to_process):
            logger.info("üéâ All files processed successfully!")
            return 0
        else:
            logger.warning(f"‚ö†Ô∏è {len(files_to_process) - success_count} files failed processing")
            return 1
    
    def _display_report(self, report: Dict[str, Any], filename: str):
        """Display deduplication report."""
        logger.info(f"üìä Deduplication Report for {filename}:")
        logger.info(f"   Original size: {report.get('original_size', 0)}")
        logger.info(f"   Final size: {report.get('final_size', 0)}")
        logger.info(f"   Removed: {report.get('total_removed', 0)}")
        logger.info(f"   Reduction: {report.get('deduplication_rate', 0):.1f}%")
        
        if 'steps' in report:
            logger.info("   Step details:")
            for step in report['steps']:
                step_name = step.get('step', 'unknown')
                removed = step.get('removed', 0)
                logger.info(f"     {step_name}: removed {removed} items")


def add_parser(subparsers, name: str) -> argparse.ArgumentParser:
    """Add the deduplicate command parser to subparsers."""
    command = DeduplicateCommand()
    return command.add_parser(subparsers, name)


def run(args: argparse.Namespace) -> int:
    """Run the deduplicate command."""
    command = DeduplicateCommand()
    return command.run(args)