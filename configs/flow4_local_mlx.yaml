# Flow4 + Augmentoolkit Configuration for LOCAL-ONLY MLX Models on Apple Silicon
# This config ensures 100% local processing with no external API dependencies

pipeline: factual-datagen-pipeline

# Disable flattening for complex nested structures
no_flatten:
  - factual_sft
  - template_kwargs
  - generic_dataset_paths
  - generic_dataset_percentages
  - other_pretrain_kwargs
  - other_finetune_kwargs
  - input_dirs

# Disable automatic training since we're focused on dataset generation
model_auto_train:
  do_train: False

# Enable automatic model running for testing (optional)
model_auto_run:
  do_run: False
  cache_dir: ./cache
  server_type: normal

# Path configuration for Flow4 integration
path:
  input_dirs:
    - path: ./output/chunks/  # Flow4 chunking output directory
      variation_generation_counts: 2  # Reduced for faster local generation
      final_system_prompt_additional_context: "You are an expert assistant focused on technical documentation and analysis."
      factual_gen_subset_size_per_way: 500  # Smaller subset for local generation
      factual_gen_use_subset: True
      rag_subset_size: 300
      rag_use_subset: True
      correction_subset_size: 300
      correction_use_subset: True
  output_dir: ./output/augmentoolkit_datasets/
  models_dir: ./models/
  huggingface_cache_dir: ./cache/huggingface

# System configuration optimized for local MLX generation
system:
  number_of_factual_sft_generations_to_do: 1  # Single generation pass for speed
  completion_mode: False
  remove_system_prompt_ratio: 0.1
  remove_thought_process_ratio: 0.1
  remove_thought_process_prompt: "Answer directly without showing your reasoning process."
  final_answer_str: "Answer:"
  generic_thought_process_on_domain_data: False  # Disabled for simplicity
  cite_sources_at_end: True
  concurrency_limit: 4  # Conservative limit for local generation
  use_stop: True
  subset_size: 50  # Small subset for testing
  use_subset: True
  chunk_size: 3000  # Reasonable chunk size for MLX models
  num_tokens_pretraining_in_sft: 500000  # Reduced for local generation
  shared_instruction: |
    You are an AI assistant specialized in technical documentation and analysis. 
    You provide clear, accurate answers based on the provided context.
    Always think carefully before responding and cite relevant sources when available.
    
    Your responses should be:
    - Accurate and factual
    - Clear and well-structured
    - Appropriately detailed for the question
    - Professional and helpful in tone

# MLX Model Configuration (100% LOCAL - Apple Silicon)
factual_sft_settings:
  factual_use_stop: True
  factual_chunk_size: 3000
  factual_completion_mode: False
  factual_small_model: mlx-community/Llama-3.2-3B-Instruct-4bit
  factual_small_api_key: local
  factual_small_base_url: local
  factual_small_mode: mlx
  factual_large_model: mlx-community/Llama-3.2-3B-Instruct-4bit
  factual_large_api_key: local
  factual_large_base_url: local
  factual_large_mode: mlx
  factual_cost_per_million_small_input: 0.0  # No cost for local models
  factual_cost_per_million_small_output: 0.0
  factual_cost_per_million_large_input: 0.0
  factual_cost_per_million_large_output: 0.0
  final_assistant_prompts_no_rag: [
    'You are a helpful AI assistant specializing in technical documentation.',
    'As an expert in technical analysis, provide clear and accurate information.',
    'You are an AI assistant focused on providing accurate technical insights.',
  ]
  items_per_conversation: 2  # Reduced for faster generation
  combine_sharegpt_target_pairs: 3

# Dataset context for Flow4 document processing
dataset_context: Technical Documentation and Analysis

# RAG data configuration for MLX (LOCAL)
rag_data:
  rag_failure_percentage: 0.30  # Lower failure rate for better quality
  rag_max_chunks: 2  # Fewer chunks for local generation
  user_format: "Human: {user}"
  system_format: "System: {system}"
  assistant_format: "Assistant: {assistant}"
  bos: "<s>"
  final_assistant_prompts: [
    'You are a helpful AI assistant specializing in technical documentation. Use the provided context: {data}',
    'Based on the following technical information, provide accurate answers: {data}',
    'Use this technical context to answer questions accurately: {data}',
  ]
  num_items_per_group: 2
  rag_skip_filter_chunks: False
  rag_small_model: mlx-community/Llama-3.2-3B-Instruct-4bit
  rag_small_api_key: local
  rag_small_base_url: local
  rag_small_mode: mlx
  rag_large_model: mlx-community/Llama-3.2-3B-Instruct-4bit
  rag_large_api_key: local
  rag_large_base_url: local
  rag_large_mode: mlx
  rag_cost_per_million_small_input: 0.0
  rag_cost_per_million_small_output: 0.0
  rag_cost_per_million_large_input: 0.0
  rag_cost_per_million_large_output: 0.0
  rag_use_stop: True
  rag_prompts: prompts_local
  rag_default_prompts: prompts_local

# Factual SFT configuration with multiple question types - LOCAL ONLY
factual_sft:
  openended:
    prompts: prompt_overrides_local/openended
    default_prompts: prompts_local
    single_turn: True
    skip_question_check: False  # Enable quality checks
    skip_answer_relevancy_check: False
    skip_answer_accuracy_check: False
    skip_repair_qa_tuples: False
    multi_source: False
  followup:
    prompts: prompt_overrides_local/followup
    default_prompts: prompts_local
    single_turn: False
    skip_question_check: False
    skip_answer_relevancy_check: False
    skip_answer_accuracy_check: False
    skip_repair_qa_tuples: False
    multi_source: False

# PDF cleaning configuration (LOCAL MLX)
pdf_cleaning:
  pdf_cleaning_chunk_size: 3000
  pdf_cleaning_small_model: mlx-community/Llama-3.2-3B-Instruct-4bit
  pdf_cleaning_large_model: mlx-community/Llama-3.2-3B-Instruct-4bit
  pdf_cleaning_small_mode: mlx
  pdf_cleaning_large_mode: mlx
  pdf_cleaning_small_base_url: local
  pdf_cleaning_large_base_url: local
  pdf_cleaning_small_api_key: local
  pdf_cleaning_large_api_key: local
  pdf_cleaning_use_stop: True
  pdf_cleaning_cost_small_input: 0.0
  pdf_cleaning_cost_small_output: 0.0
  pdf_cleaning_cost_large_input: 0.0
  pdf_cleaning_cost_large_output: 0.0
  pdf_cleaning_prompts: prompts_local
  pdf_cleaning_default_prompts: prompts_local

# Correction pipeline configuration - LOCAL ONLY
correction_pipeline:
  correction_use_subset: True  # Enable subset for local processing
  correction_subset_size: 300
  correction_chunk_size: 3000
  correction_small_model: mlx-community/Llama-3.2-3B-Instruct-4bit
  correction_small_api_key: local
  correction_small_base_url: local
  correction_small_mode: mlx
  correction_large_model: mlx-community/Llama-3.2-3B-Instruct-4bit
  correction_large_api_key: local
  correction_large_base_url: local
  correction_large_mode: mlx
  correction_cost_per_million_small_input: 0.0
  correction_cost_per_million_small_output: 0.0
  correction_cost_per_million_large_input: 0.0
  correction_cost_per_million_large_output: 0.0
  correction_prompt_template: "{% for message in messages %}{% if (message['role'] == 'system') %}{{message['content'] + '\\n'}}{% elif (message['role'] == 'user') %}{{'Human: ' + message['content'] + '\\n'}}{% elif message['role'] == 'assistant' %}{{'Assistant: ' + message['content'] + '\\n'}}{% endif %}{% endfor %}"
  correction_use_stop: True
  correction_completion_mode: False
  correction_prompts: prompts_local
  correction_default_prompts: prompts_local

# Final dataset configuration
final_datasaving_settings:
  template: "chatml"  # Use ChatML format for instruction tuning
  template_kwargs: {}
  generic_dataset_paths: []  # No generic datasets for Flow4 focus
  generic_dataset_percentages: []
  max_samples_per_dataset: 10000  # Reasonable limit for local processing
  minimum_generic_sft: 0

# Model training disabled for dataset generation focus
model_training:
  base_model: mlx-community/Llama-3.2-3B-Instruct-4bit
  context_length: 8192
  is_mistral_derived_model: False
  other_pretrain_kwargs: {}
  other_finetune_kwargs: {}

# Disable LLM-based PDF processing for simplicity
do_not_use_llm_for_pdf_processing: True